# AI Service Configuration

# LLM Provider Configuration
# Opções: 'ollama' (padrão, gratuito) ou 'openai' (pago)
LLM_PROVIDER=ollama

# Modelo LLM a usar (deixe vazio para usar padrão do provider)
LLM_MODEL=

# Temperatura do modelo (0 = determinístico, valores maiores = mais criativo)
LLM_TEMPERATURE=0

# ============================================================================
# Ollama Configuration (LLM Local Gratuito)
# ============================================================================
# URL base do Ollama (padrão: http://localhost:11434)
OLLAMA_BASE_URL=http://localhost:11434

# Modelo Ollama a usar (recomendados: llama3, mistral, phi3)
# Para listar modelos disponíveis: ollama list
OLLAMA_MODEL=llama3

# ============================================================================
# OpenAI Configuration (Fallback Opcional)
# ============================================================================
# API Key da OpenAI (necessária apenas se LLM_PROVIDER=openai)
OPENAI_API_KEY=

# Modelo OpenAI a usar (padrão: gpt-4o-mini)
OPENAI_MODEL=gpt-4o-mini

# ============================================================================
# Backend API Configuration
# ============================================================================
# URL do backend API
BACKEND_API_URL=http://localhost:8000
